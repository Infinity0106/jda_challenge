{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/infinity0106/Documents/Personal/jda_challenge\n"
     ]
    }
   ],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "# ms-python.python added\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), '..'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, TimeDistributed, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JDAKeras():\n",
    "    def __init__(self):\n",
    "        self.regressor = Sequential()\n",
    "\n",
    "    def add_layers(self, input, output):\n",
    "        # Initialising the RNN\n",
    "        self.regressor.add(LSTM(units=50,\n",
    "                                activation='tanh',\n",
    "                                input_shape=input,\n",
    "                                return_sequences=True))\n",
    "        self.regressor.add(Dropout(0.6))\n",
    "\n",
    "        self.regressor.add(LSTM(units=50,\n",
    "                                activation='tanh'))\n",
    "        self.regressor.add(Dropout(0.6))\n",
    "\n",
    "        # Adding the output layer\n",
    "        # self.regressor.add(Lambda(lambda x: x[:, -output:, :])) # less output size\n",
    "        self.regressor.add(Dense(1))  # same size output\n",
    "\n",
    "        print(self.regressor.summary())\n",
    "\n",
    "    def optimizers(self):\n",
    "        # Compiling the RNN\n",
    "        self.regressor.compile(\n",
    "            optimizer=\"adam\", loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "\n",
    "    def train(self, X_train, y_train, validation_data, epochs=100, batch_size=5000, currency=\"EURUSD1\"):\n",
    "        # Check points\n",
    "        filepath = currency + \\\n",
    "            \"-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1,\n",
    "                                     save_best_only=True, save_weights_only=False, period=1, mode='max')\n",
    "        callbacks_list = [checkpoint]\n",
    "        # Fitting the RNN to the Training set\n",
    "        self.regressor.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                           callbacks=callbacks_list, validation_data=validation_data)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.regressor.save(f\"{name}.h5\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.regressor = load_model(path)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-be57ae97db13>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-be57ae97db13>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = pd.read_pickle(\"./preprocesed.pkl\")\n",
    "    X = df.drop([\"sa_quantity\"], axis=1)\n",
    "    Y = df[\"sa_quantity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = SGDRegressor(max_iter=100, tol=None, verbose=1, \n",
    "                         validation_fraction=0.3, early_stopping=True,\n",
    "                        alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 7.92, NNZs: 513, Bias: -0.004697, T: 4954194, Avg. loss: 0.247182\n",
      "Total training time: 54.47 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.02, NNZs: 513, Bias: 0.007797, T: 9908388, Avg. loss: 0.237949\n",
      "Total training time: 268.38 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.002133, T: 14862582, Avg. loss: 0.237700\n",
      "Total training time: 446.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.003424, T: 19816776, Avg. loss: 0.237749\n",
      "Total training time: 632.34 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.007943, T: 24770970, Avg. loss: 0.237700\n",
      "Total training time: 842.57 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.002423, T: 29725164, Avg. loss: 0.237729\n",
      "Total training time: 1029.45 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.001590, T: 34679358, Avg. loss: 0.237695\n",
      "Total training time: 1222.76 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.002376, T: 39633552, Avg. loss: 0.237725\n",
      "Total training time: 1414.84 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.008894, T: 44587746, Avg. loss: 0.237695\n",
      "Total training time: 1601.52 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000820, T: 49541940, Avg. loss: 0.237717\n",
      "Total training time: 1791.83 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.002591, T: 54496134, Avg. loss: 0.237712\n",
      "Total training time: 1990.22 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000124, T: 59450328, Avg. loss: 0.237674\n",
      "Total training time: 2185.81 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.002986, T: 64404522, Avg. loss: 0.237723\n",
      "Total training time: 2385.51 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.001352, T: 69358716, Avg. loss: 0.237679\n",
      "Total training time: 2602.74 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 8.02, NNZs: 513, Bias: -0.002913, T: 74312910, Avg. loss: 0.237736\n",
      "Total training time: 2803.13 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.004404, T: 79267104, Avg. loss: 0.237664\n",
      "Total training time: 3019.79 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.004807, T: 84221298, Avg. loss: 0.237694\n",
      "Total training time: 3251.14 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.004587, T: 89175492, Avg. loss: 0.237670\n",
      "Total training time: 3440.52 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.003910, T: 94129686, Avg. loss: 0.237705\n",
      "Total training time: 3627.45 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.009717, T: 99083880, Avg. loss: 0.237701\n",
      "Total training time: 3805.10 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.007821, T: 104038074, Avg. loss: 0.237647\n",
      "Total training time: 3993.97 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.002628, T: 108992268, Avg. loss: 0.237679\n",
      "Total training time: 4182.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.004259, T: 113946462, Avg. loss: 0.237673\n",
      "Total training time: 4376.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.002446, T: 118900656, Avg. loss: 0.237669\n",
      "Total training time: 4561.37 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.004796, T: 123854850, Avg. loss: 0.237721\n",
      "Total training time: 4748.29 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 8.01, NNZs: 513, Bias: -0.006234, T: 128809044, Avg. loss: 0.237687\n",
      "Total training time: 4950.94 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.005237, T: 133763238, Avg. loss: 0.237648\n",
      "Total training time: 5145.17 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.008268, T: 138717432, Avg. loss: 0.237674\n",
      "Total training time: 5344.78 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.002718, T: 143671626, Avg. loss: 0.237711\n",
      "Total training time: 5514.32 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.006072, T: 148625820, Avg. loss: 0.237651\n",
      "Total training time: 5690.78 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.001285, T: 153580014, Avg. loss: 0.237704\n",
      "Total training time: 5865.11 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.004788, T: 158534208, Avg. loss: 0.237657\n",
      "Total training time: 6074.31 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.001235, T: 163488402, Avg. loss: 0.237694\n",
      "Total training time: 6264.32 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.006666, T: 168442596, Avg. loss: 0.237695\n",
      "Total training time: 6454.76 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.004291, T: 173396790, Avg. loss: 0.237653\n",
      "Total training time: 6664.30 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000862, T: 178350984, Avg. loss: 0.237671\n",
      "Total training time: 6846.31 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.009114, T: 183305178, Avg. loss: 0.237676\n",
      "Total training time: 7053.12 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.006672, T: 188259372, Avg. loss: 0.237645\n",
      "Total training time: 7236.46 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.005385, T: 193213566, Avg. loss: 0.237685\n",
      "Total training time: 7436.08 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.004534, T: 198167760, Avg. loss: 0.237670\n",
      "Total training time: 7633.36 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.006255, T: 203121954, Avg. loss: 0.237674\n",
      "Total training time: 7831.99 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.003589, T: 208076148, Avg. loss: 0.237679\n",
      "Total training time: 8012.74 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.000973, T: 213030342, Avg. loss: 0.237676\n",
      "Total training time: 8238.44 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.001061, T: 217984536, Avg. loss: 0.237652\n",
      "Total training time: 8434.93 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.005231, T: 222938730, Avg. loss: 0.237691\n",
      "Total training time: 8613.78 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.002596, T: 227892924, Avg. loss: 0.237683\n",
      "Total training time: 8800.68 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.001721, T: 232847118, Avg. loss: 0.237668\n",
      "Total training time: 9016.98 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 8.01, NNZs: 513, Bias: -0.002581, T: 237801312, Avg. loss: 0.237685\n",
      "Total training time: 9196.54 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.005293, T: 242755506, Avg. loss: 0.237664\n",
      "Total training time: 9384.50 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.000818, T: 247709700, Avg. loss: 0.237642\n",
      "Total training time: 9566.47 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.000020, T: 252663894, Avg. loss: 0.237680\n",
      "Total training time: 9775.84 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.006081, T: 257618088, Avg. loss: 0.237691\n",
      "Total training time: 9965.64 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.002260, T: 262572282, Avg. loss: 0.237634\n",
      "Total training time: 10161.83 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.003148, T: 267526476, Avg. loss: 0.237648\n",
      "Total training time: 10339.40 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.001755, T: 272480670, Avg. loss: 0.237667\n",
      "Total training time: 10528.81 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.002333, T: 277434864, Avg. loss: 0.237706\n",
      "Total training time: 10724.39 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.003428, T: 282389058, Avg. loss: 0.237655\n",
      "Total training time: 10897.50 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.003489, T: 287343252, Avg. loss: 0.237663\n",
      "Total training time: 11086.19 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.001000, T: 292297446, Avg. loss: 0.237668\n",
      "Total training time: 11275.14 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.001630, T: 297251640, Avg. loss: 0.237680\n",
      "Total training time: 11465.09 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000211, T: 302205834, Avg. loss: 0.237650\n",
      "Total training time: 11648.37 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.004968, T: 307160028, Avg. loss: 0.237655\n",
      "Total training time: 11824.23 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.003191, T: 312114222, Avg. loss: 0.237663\n",
      "Total training time: 12020.09 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.002705, T: 317068416, Avg. loss: 0.237668\n",
      "Total training time: 12222.43 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.003762, T: 322022610, Avg. loss: 0.237691\n",
      "Total training time: 12398.62 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.001714, T: 326976804, Avg. loss: 0.237668\n",
      "Total training time: 12590.99 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.010397, T: 331930998, Avg. loss: 0.237616\n",
      "Total training time: 12788.72 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.001524, T: 336885192, Avg. loss: 0.237707\n",
      "Total training time: 12972.23 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.004265, T: 341839386, Avg. loss: 0.237651\n",
      "Total training time: 13176.66 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.003574, T: 346793580, Avg. loss: 0.237688\n",
      "Total training time: 13368.59 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.003333, T: 351747774, Avg. loss: 0.237633\n",
      "Total training time: 13569.03 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.012843, T: 356701968, Avg. loss: 0.237673\n",
      "Total training time: 13775.85 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.004261, T: 361656162, Avg. loss: 0.237675\n",
      "Total training time: 13968.72 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000568, T: 366610356, Avg. loss: 0.237650\n",
      "Total training time: 14144.88 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.006630, T: 371564550, Avg. loss: 0.237671\n",
      "Total training time: 14341.47 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.000231, T: 376518744, Avg. loss: 0.237653\n",
      "Total training time: 14521.53 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.001635, T: 381472938, Avg. loss: 0.237675\n",
      "Total training time: 14706.19 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.002283, T: 386427132, Avg. loss: 0.237641\n",
      "Total training time: 14907.63 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.008537, T: 391381326, Avg. loss: 0.237670\n",
      "Total training time: 15118.74 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.006775, T: 396335520, Avg. loss: 0.237669\n",
      "Total training time: 15325.64 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.001783, T: 401289714, Avg. loss: 0.237624\n",
      "Total training time: 15520.71 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.009659, T: 406243908, Avg. loss: 0.237658\n",
      "Total training time: 15710.24 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.002441, T: 411198102, Avg. loss: 0.237692\n",
      "Total training time: 15901.75 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000230, T: 416152296, Avg. loss: 0.237647\n",
      "Total training time: 16095.73 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.002334, T: 421106490, Avg. loss: 0.237660\n",
      "Total training time: 16293.78 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 7.99, NNZs: 513, Bias: -0.004559, T: 426060684, Avg. loss: 0.237643\n",
      "Total training time: 16488.68 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.003487, T: 431014878, Avg. loss: 0.237680\n",
      "Total training time: 16680.70 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000832, T: 435969072, Avg. loss: 0.237664\n",
      "Total training time: 16879.81 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 7.99, NNZs: 513, Bias: 0.000669, T: 440923266, Avg. loss: 0.237634\n",
      "Total training time: 17077.09 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.002712, T: 445877460, Avg. loss: 0.237680\n",
      "Total training time: 17275.92 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.003119, T: 450831654, Avg. loss: 0.237675\n",
      "Total training time: 17463.10 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.003417, T: 455785848, Avg. loss: 0.237637\n",
      "Total training time: 17643.35 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.003268, T: 460740042, Avg. loss: 0.237653\n",
      "Total training time: 17855.29 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.007663, T: 465694236, Avg. loss: 0.237668\n",
      "Total training time: 18036.61 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.004852, T: 470648430, Avg. loss: 0.237678\n",
      "Total training time: 18223.84 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 8.00, NNZs: 513, Bias: -0.001952, T: 475602624, Avg. loss: 0.237644\n",
      "Total training time: 18427.50 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.002380, T: 480556818, Avg. loss: 0.237667\n",
      "Total training time: 18626.14 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.005293, T: 485511012, Avg. loss: 0.237645\n",
      "Total training time: 18817.95 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 8.00, NNZs: 513, Bias: 0.000798, T: 490465206, Avg. loss: 0.237669\n",
      "Total training time: 19017.21 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 8.01, NNZs: 513, Bias: 0.002102, T: 495419400, Avg. loss: 0.237685\n",
      "Total training time: 19224.03 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.001, average=False, early_stopping=True, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=100,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=None, validation_fraction=0.3, verbose=1,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6819911521688934\n"
     ]
    }
   ],
   "source": [
    "    print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_sklearn.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "     dump(model, 'model_sklearn.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tmp_model = load('model_sklearn.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_jupy = model.predict(x_test)\n",
    "    y_load = tmp_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "    print(mean_squared_error(y_jupy, y_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1697400, 513)\n"
     ]
    }
   ],
   "source": [
    "    df = pd.read_pickle(\"./preprocesed_pred.pkl\")\n",
    "    X = df.drop([\"sa_quantity\",\"location\",\"product\",\"date\",\n",
    "                 \"temp_max\",\"temp_min\",\"event\"], axis=1)\n",
    "    print(X.shape)\n",
    "    Y = df[\"sa_quantity\"]\n",
    "    y_pred = tmp_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     7.0\n",
      "1    12.0\n",
      "2     9.0\n",
      "3     7.0\n",
      "4    20.0\n",
      "Name: sa_quantity, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    df = pd.read_csv(\"./server/input_data_pred.csv\")\n",
    "    y_pred = np.absolute(np.round((y_pred * 13.035948087840143) + 8.604585583447076))\n",
    "    df[\"sa_quantity\"]=y_pred\n",
    "    print(df[\"sa_quantity\"].head())\n",
    "    np.max(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    out_df = df[[\"location\",\"product\",\"date\",\"temp_mean\",\n",
    "                 \"temp_max\",\"temp_min\",\"sunshine_quant\",\"event\",\"price\",\"sa_quantity\"]]\n",
    "    out_df.to_csv(\"./server/output_data_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = JDAKeras()\n",
    "    model.load_model('./server/model_jda.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-468935090df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fce069a077df>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X_test)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    X=X.to_numpy()\n",
    "    X=X[:,:,np.newaxis]\n",
    "    y_pred = model.predict(X)\n",
    "    print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_pred = y_pred.squeeze()\n",
    "    y_pred_norm = np.round((y_pred * 0.9999999999999989) + 2.5899186438952162e-18)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
